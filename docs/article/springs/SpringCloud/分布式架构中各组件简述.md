# 分布式架构中各组件简述



# 服务注册

## Eureka

## Consul



# 服务调用

## Ribbon

> 已经进入维护阶段

- Spring Cloud Ribbon 是基于 Netflix Ribbon 实现的一套**客户端负载均衡工具**

- 主要功能是提供客户端软件**负载均衡算法**和**服务调用**
- Ribbon 客户端提供一系列完善的配置项：如连接超时、重试等
- 在配置文件中列出 LB（Load Balancer）后面所有的所有机器，Ribbon 都会自动基于某些规则（轮询，随机连接等）去连接这些机器



<br/>

### Load Balancer

- 将用户请求分摊到多个服务上，达到系统的高可用
- 常见的负载均衡工具还有 Nginx、LVS、硬件 F5 等
- Nginx 是服务器负载均衡，客户端所有请i去都会交给 Nginx，然后由 Nginx 实现转发请求
- Ribbon 是本地负载均衡，在调用微服务接口时，会在注册中心上在获取到注册信息服务列表后缓存到 JVM 本地，从而在本地实现 RPC 远程服务调用



<br/>

### 负载均衡类型

- 集中式：服务的消费方和提供方之间使用独立的 LB 设施（Nginx、F5 等），由该设施负责把请求通过某种策略发送给服务的提供方
- 进程内：将 LB 逻辑集成到消费方，消费方从服务注册中心获取可用的地址，然后从这些地址中选择出一台合适的服务器。Ribbon 就属于进程内 LB，它只是一个类库，集成于消费方进程，消费方通过它来获取服务提供方的地址



<br/>

### 总结

- Ribbon 是一个软负载均衡的客户端组件
- Ribbon 可以和其他所需请求的客户端结合使用，和 Eureka 结合就是其中一个实例（较新版的 Spring Cloud Eureka 自带了 Ribbon）



<br/>

## OpenFeign

- 一个声明式 WebServeice 客户端，使用 Feign 编写可以让客户端更加简单
- 使用方法式定义一个服务接口然后再上面添加注解
- Feign 支持可插拔式的编码器和解码器
- Spring Cloud 对 Feign 进行了封装，即 OpenFeign，使得其支持了 Spring MVC 标准注解和 HttpMessageConverters
- OpenFiegn 可以与 Eureka 和 Ribbon 组合使用以支持负载均衡



<br/>

### Feign 的作用

- 使用 Ribbon 时要和 RestTemplate 一起使用，即使用 RestTemplate 对 http 请求进行封装，形成一套模板化的调用方法
- 但是在实际开发中，对服务依赖的调用可能不止一处，往往是一个接口会被多出调用，所以通常会对每个微服务自行封装一些客户端类来包装这些依赖服务的调用
- Feign 在上述基础上做了进一步封装，由它来定义和实现依赖服务接口的定义
- 在 Feign 的是线下，我们只需要创建一个接口并使用注解的方式来配置它；以前是在 Mapper 接口上标注 Mapper 接口，现在是在一个微服务接口上标注一个 Feign 注解即可完成对服务提供方的接口绑定，即简化了 Ribbon 的使用
- Feign 已经集成了 Ribbon，自带负载均衡

<br/>



---



<br/>

# 服务降级

**服务雪崩：**

- 多个微服务之间调用可能形成多条调用链，如果扇出的链路上某个微服务的响应时间太长或者服务不可用，那么最开始的服务就会占用越来越多的系统资源，进而引入系统崩溃
- 对于高流量的应用来说，单一的后端依赖可能会导致所有服务器上的所有资源在几秒钟之内饱和
- 比失败更糟糕的情况是导致服务之间的延迟增加、备份队列、线程和其他系统资源紧张，导致整个系统发生更多的级联故障
- 对于这些故障和延迟都需要进行隔离和管理



## Hystrix

> 需要手动搭建监控平台，没有一套 Web 界面能让我们进行更加细粒度的配置

- 一个用于处理分布式系统的**延迟**和**容错**的开源库
- 保证在一个依赖出问题的情况下，不会导致整体服务失败，避免了级联故障，提高了分布式系统的弹性
- 断路器本身是一种开关装置，当某个服务单元发生故障之后，通过断路器的故障监控（类似熔断保险丝），向调用方返回一个符合预期的、可处理的备选响应（FallBack），而不是长时间的等待或者抛出调用方无法处理的异常
- 保障了服务调用方的线程不会被长时间且不必要地占用，从而避免了故障在分布式系统中的蔓延乃至雪崩

**服务降级（Fallback）：**

- 向调用方返回一个符合预期的、可处理的备选响应（FallBack）
- 发生服务降级的情况：
  - 程序运行异常
  - 超时
  - 服务熔断触发服务降级
  - 线程池/信号量打满

**服务熔断（Break）：**

- 达到最大访问量后，直接拒绝访问，然后调用服务降级方法返回提示
- 熔断机制是应对**雪崩效应**的一种微服务链路保护机制
- 当扇出链路的某个微服务出错不可用或者响应时间太长就会进行服务降级，进而熔断该节点的微服务的调用，快速返回错误的响应信息
- 当检测到该节点微服务调用响应正常后，恢复调用链路
- 在 Spring Cloud 中，Hystrix 会监控微服务间的调用状况，以此实现熔断机制
- 当失败的调用到达一定的阈值后（默认是 5 秒内 20 次调用失败），就会启动熔断机制
- 熔断状态：
  - 打开：请求不再调用当前服务，内部设置始终一般为 MTTR（平均故障处理时间），当打开时长达到所设时钟则进入半熔断状态
  - 关闭：不会对服务进行熔断
  - 半开：部分请求根据规则调用当前服务，如果请求成功且符合规则则认为当前服务恢复正常，关闭熔断
- 断路器的三个重要参数：
  - 快照时间窗口：断路器确定是否打开需要统计一些请求和错误数据，统计的时间范围就是快照时间窗口，默认为最近的 10 秒
  - 请求总数阈值：在快照时间窗口内，必须满足请求总数阈值才有资格熔断。默认为 20，意味着在 10 秒内，如果该 Hystrix 命令的调用次数不足 20 次，即使所有的请求都超时或者因为其他的原因而失败，断路器也不会打开
  - 错误百分比阈值：当请求总数在快照时间窗口内超过了阈值，比如发生了 30 次调用，如果在这 30 次调用中，有 15 次发生了超时异常，也就是超过了 50% 的错误百分比，那么在默认设定的 50% 阈值的情况下，断路器就会打开
- Hystrix 的自动恢复功能：
  - 当断路器打开时，会对主逻辑进行熔断，之后会启动一个休眠时间窗口，在这个时间窗口内，降级逻辑临时成为逻辑
  - 当休眠窗口期到期，断路器会进入半开状态，释放一次请求到原来的主逻辑上，如果此次请求正常返回，那么断路器将关闭，主逻辑恢复
  - 如果这次请求依然有问题，那么还是保持打开的状态，休眠时间窗口重新计时

**服务限流（Flowlimit）：**

- 在类似秒杀的高并发场景下，严禁流量瞬间涌入，此时请求需要排队有序进入



<br/>

## Sentinel

> 独立的组件，有 Web 界面，可以更加细粒度地配置

![image-20221023112720793](C:\Users\gzw\AppData\Roaming\Typora\typora-user-images\image-20221023112720793.png)

### 流控规则

| 序号 |       属性        | 说明                                                         |
| :--: | :---------------: | :----------------------------------------------------------- |
|  1   |      资源名       | 唯一名称，默认为请求路径                                     |
|  2   |     针对来源      | Sentinel 可以针对调用者进行限流，填写微服务名称，默认为 default（不区分来源） |
|  3   | 阈值类型/单机阈值 | 1. QPS（每秒请求数量）：当调用该 API 的 QPS 达到阈值时进行限流<br />2. 线程数：当调用该 API 的线程数达到阈值时进行限流 |
|  4   |     是否集群      | 不需要集群                                                   |
|  5   |     流控模式      | 1. 直接：API 达到限流条件时直接限流<br />2. 关联：当关联的资源达到阈值时，限流自己（A 关联 B，B 达到阈值，A 挂）<br />3. 链路：只记录指定链路上的流量（API 级别），即指定资源入口进来的流量，如果达到阈值就进行限流 |
|  6   |     流控效果      | 1. 快速失败：直接失败抛异常<br />2. Warm up：根据 codeFactor（冷加载因子，默认 3）的值，从阈值/codeFactor，进过预热时长才达到设置的 QPS 阈值<br />3. 排队等待：让请求匀速通过，阈值类型必须设置成 QPS 否则无效 |



### 降级规则

> Sentinel 熔断降级会在调用链路中某个资源出现不稳定时，对这个资源的调用进行限制，让请求快速失败，避免影响到其他的资源二导致级联错误；当资源被降级后，在接下来的降级时间窗口内，对该资源的调用都自动熔断（默认是抛出 DegradeException）

| 序号 |           属性           |                             说明                             |
| :--: | :----------------------: | :----------------------------------------------------------: |
|  1   | RT（平均响应时间，秒级） | 超出阈值且在时间窗口内通过的请求大于等于 5 时，触发降级<br />窗口期过后关闭断路器<br />RT 最大为 4900（更大的需要设置 -Dcsp.sentinel.statistic.max.rt=xxx） |
|  2   |     异常比例（秒级）     | QPS 大于等于 5 且异常比例（秒级统计）超过阈值时，触发降级<br />时间窗口结束后，关闭降级 |
|  3   |     异常数（分钟级）     | 异常数（分钟统计）超过阈值时，触发降级<br />时间窗口结束后，关闭降级 |



<br/>

### 热点规则

p124



<br/>

# 服务网关

**Gateway（Zuul 不讨论）：**

- 基于 Spring + S pring Boot 和 Project Reator 等技术开发的网关，为微服务架构提供一种简单有效的统一的 API 路由管理方式
- 基于 WebFlux 框架实现的，底层使用了高性能的 Reactor 模式通信框架 Netty，所以 Gateway 是异步非阻塞模型
- 目标是提供统一的路由方式，且是基于 Filter 链的方式提供的网关的基本功能（如：安全、监控、指标、限流）
- Route 路由：构建网关的基本模块，由 ID、目标 URI 、一系列的断言和过滤器组成，如果断言为真就匹配该路由
- Filter 过滤：GatewayFilter 实例，可以在请求前后做一些操作





# 服务配置中心

- 为微服务提供集中化的外部配置支持，配置服务器为各个不同的微服务应用的所有环境提供一个中心化的外部配置
- 配置中心支持动态化的配置更新，可以分环境部署
- 运行期间可以动态调整配置，不需要在每个服务部署的机器上编写配置文件，服务会向配置中心统一拉取配置自己的信息
- 当配置发生改动时，服务不需要重启即可感知到配置的变化并应用新的配置
- 配置信息以 RESTful 风格的接口暴露

**配置文件分级：**

- `application.yml` 是用户级的资源配置项；`bootstrap.yml` 是系统级的配置项，优先级更高
- Spring Cloud 会创建一个 `Bootstrap Context` 作为 Spring 应用的 `Application Context` 的父上下文；两个上下文共享一个从外部获取的 `Environment`
- 初始化时  `Bootstrap Context`  负责从外部源加载配置属性并解析配置
- 两个上下文有着不同的约定，保证它们之间的配置分离
- `Bootstrap Context` 属性有高优先级，默认情况下不会被本地配置覆盖
- 跟配置中心对接的客户端中的配置文件需要改成 `bootstrap.yml



# 消息总线

- 这里说的是 `Spring Cloud Bus`
- 在微服务架构的系统中，通常会使用轻量级的消息代理来构建一个公用的消息主题，并让系统中所有微服务实例都链接上来。由于该主题中产生的消息会被所有实例监听和消费，所以称之为消息总线
- 基本原理是客户端实例都会监听消息队列中的同一个 `topic`（默认是 `Spring Cloud Bus`），当一个服务刷新数据时，它会把这个消息放到该主题中，这样其他监听同一个主题的服务就能得到新的通知，并更新自身的状态
- 可以配合 `Spring Cloud Config` 实现真正的动态配置刷新
- 支持两种消息代理：`RabbitMQ` 和 `Kafaka`
- `Spring Cloud Bus` 是将分布式系统的节点与轻量级消息系统链接起来的框架，整合了 Java 事件处理机制和消息中间件的功能
- 能够管理和传播分布式系统间的消息，就像一个分布式执行器，可以用于广播状态更改、事件推送等，也可以作为微服务间的通信通道

**两种触发方式：**

- 利用消息总线触发一个客户端 `/bus/refresh` 端点从而刷新所有客户端的配置
- 利用消息总线触发一个服务端  `/bus/refresh` 端点从而刷新所有客户端的配置

- 注意：需要给配置中心的服务端和客户端都加上消息总线的支持

**使用客户端通知的弊端：**

- 打破了微服务的职责单一性，因为客户端本身是业务模块，本就不应该承担配置刷新的职责
- 破坏了微服务个节点的平衡性
- 存在其他的局限性，如微服务在迁移的时候，网络地址是常常发生改变的，如果这时候想要刷新可能需要更多的修改



# 消息驱动

<img src="C:\MyDisk\B-Data\Record\Note\WorkingArea\CodingStudy\分布式\分布式架构中各组件简述.assets\SpringCloudStream.png" alt="SpringCloudStream" style="zoom: 80%;" />

> Spring Cloud Stream，Binder 对象是关键

- 构建消息驱动微服务的框架，屏蔽底层消息中间件的差异，降低切换成本，统一消息的编程模型
- 应用程序通过 inputs 或者 outputs 来与 Spring Cloud Strem 中的 binder 对象交互
- binder 对象负责与消息中间件交互，我们只需要配置绑定即可，即只需要搞清楚如何与 Stream 交互就可以方便使用消息驱动的方式
- 通过 Spring Integration 来连接消息代理中间件以实现消息事件驱动
- Stream 为一些供应商的消息中间件产品提供了个性化的自动化配置实现，引用了发布-订阅、消费组、分区的三个核心概念

**引入了 Stream 之后：**

- Stream 完成了大一统的任务（虽然现在只能选择 RabbitMQ 或者 Kafaka）
- 绑定器作为中间层能够完美实现应用程序与消息中间件之间细节的隔离，通过向应用程序暴露统一的 Channel，使得应用程序之间不需要再考虑各种不同的消息中间件的实现

**Stream 标准流程：**

- Binder：方便连接中间件，可以屏蔽差异
- Channel：对 Queue 的一种抽象，在消息通讯系统中就是实现存储和转发的媒介，通过 Channel 对队列进行配置
- Source 和 Sink：可以理解为参照对象是 Stream 自身，从 Stream 发布消息就是输出，接收消息就是输入





# 链路追踪

- 在微服务框架中，一个由客户端发起的请求在后台系统中会经过多个不同的服务节点来协同产生最后的请求结果
- 每一个请求都会形成一条复杂的分布式服务调用链路，链路中的任何一环出现高延时或错误都会引起整个请求的失败
- Spring Cloud Sleuth（用于收集）提供了一套完整的服务追踪的解决方案，并且兼容了 zipkin（用于展示）
- Sleuth 中一条请求链路通过 Trace Id 唯一标识，用 Span 标识发起的请求消息，各个 Span 通过 Parent Id 关联起来
  - Trace：类似于树结构的 Span 集合，表示一条调用链路，存在唯一标识
  - Span：表示调用链路的来源，可以理解成一次请求信息





# Nacos

> 相当于 Eureka + Config + Bus，Nacos 支持 AP、CP 的切换

## Nacos 配置中心

![Nacos命名空间](C:\MyDisk\B-Data\Excalidraw\Spring\Images\Nacos命名空间.png)

- 配置中心使用 Namespace 命名空间隔离不同的配置，内部使用 Group 和 DataID 从逻辑上区分多个目标对象；默认的为：Namespace=public，Group=DEFAULT_GROUP，默认的集群是 DEFAULT
- 读取不同的配置有三种方式：
  - 通过 DataID 区分
  - 通过 Group 区分
  - 通过 Namespace 区分



## 持久化配置

- Nacos 默认使用的是嵌入式数据（derby）库实现数据的存储
- 集群模式下如果还使用默认持久化配置会存在数据一致性问题
- 集群模式下应该使用集中式存储的方式，目前只支持 MySQL

```properties
# 持久化配置
spring.datasource.platform=mysql
### Count of DB:
db.num=1
### Connect URL of DB:
db.url.0=jdbc:mysql://127.0.0.1:3306/nacos_config?characterEncoding=utf8&connectTimeout=1000&socketTimeout=3000&autoReconnect=true
db.user.0=root
db.password.0=123456
```



## Linux 集群搭建

1. 修改 `Nacos` 的 `conf` 文件夹下的 `cluster.conf` 为集群的 `ip:port`，注意 其中的 ip 必须为 `hostname -i` 或 `ip addr ens33`  中的：

```shell
192.168.30.100:3333
192.168.30.100:4444
192.168.30.100:5555
```



2. 修改 `nacos/bin/startup.sh`：

```sh
while getopts ":m:f:s:c:p:i:" opt
do
    case $opt in
        m)
            MODE=$OPTARG;;
        f)
            FUNCTION_MODE=$OPTARG;;
        s)
            SERVER=$OPTARG;;
        c)
            MEMBER_LIST=$OPTARG;;
        p)
            EMBEDDED_STORAGE=$OPTARG;;
        i)
        	# 主要是加上 i: 和下面这一行，就是配置不同的端口启动
            PORT=$OPTARG;;
        ?)
        echo "Unknown parameter"
        exit 1;;
    esac
done
# start
echo "$JAVA ${JAVA_OPT}" > ${BASE_DIR}/logs/start.out 2>&1 &
# 加上 -Dserver.port=${PORT}
nohup $JAVA -Dserver.port=${PORT}  ${JAVA_OPT} nacos.nacos >> ${BASE_DIR}/logs/start.out 2>&1 &
echo "nacos is starting，you can check the ${BASE_DIR}/logs/start.out"
```



3. 如果无法启动可能是内存分配过小，同样是修改 `nacos/bin/startup.sh`：

```sh
if [[ "${MODE}" == "standalone" ]]; then
	# 下面一行修改大一点，例如：
	JAVA_OPT="${JAVA_OPT} -Xms2g -Xmx2g -Xmn1g"
    JAVA_OPT="${JAVA_OPT} -Dnacos.standalone=true"
else
    if [[ "${EMBEDDED_STORAGE}" == "embedded" ]]; then
        JAVA_OPT="${JAVA_OPT} -DembeddedStorage=true"
    fi
    JAVA_OPT="${JAVA_OPT} -server -Xms2g -Xmx2g -Xmn1g -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=320m"
    JAVA_OPT="${JAVA_OPT} -XX:-OmitStackTraceInFastThrow -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=${BASE_DIR}/logs/java_heapdump.hprof"
    JAVA_OPT="${JAVA_OPT} -XX:-UseLargePages"
```



4. 启动集群：

注意：较高版本的 Nacos 可能无法在同一台机器的同一个目录下启动集群，此时只需要复制配置好的 Nacos 的整个目录即可

```sh
# 如果是复制了三份，就分别进入 bin 目录，然后分别运行对应的启动命令
./startup.sh -i 3333
./startup.sh -i 4444
./startup.sh -i 5555
```

![image-20221022172459669](C:\Users\gzw\AppData\Roaming\Typora\typora-user-images\image-20221022172459669.png)



5. 修改 `nginx`  配置文件后，进入 `nginx/sbin` 执行：`./nginx -c /usr/local/nginx/conf/nginx.conf`

```properties
upstream nacosCluster {
    server localhost:3333;
    server localhost:4444;
    server localhost:5555;
}

server {
    listen          1111;
    server_name     localhost;

    location / {
    	proxy_pass http://nacosCluster/;
    }
}
```
